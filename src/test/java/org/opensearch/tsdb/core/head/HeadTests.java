/*
 * SPDX-License-Identifier: Apache-2.0
 *
 * The OpenSearch Contributors require contributions made to
 * this file be licensed under the Apache-2.0 license or a
 * compatible open source license.
 */
package org.opensearch.tsdb.core.head;

import org.apache.lucene.index.BinaryDocValues;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.LeafReaderContext;
import org.apache.lucene.index.NumericDocValues;
import org.apache.lucene.index.ReaderManager;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.MatchAllDocsQuery;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.util.BytesRef;
import org.mockito.Mockito;
import org.opensearch.common.SuppressForbidden;
import org.opensearch.common.settings.Settings;
import org.opensearch.common.unit.TimeValue;
import org.opensearch.core.index.shard.ShardId;
import org.opensearch.index.engine.Engine;
import org.opensearch.index.engine.TSDBEmptyLabelException;
import org.opensearch.index.engine.TSDBOutOfOrderException;
import org.opensearch.test.OpenSearchTestCase;
import org.opensearch.threadpool.FixedExecutorBuilder;
import org.opensearch.threadpool.TestThreadPool;
import org.opensearch.threadpool.ThreadPool;
import org.opensearch.tsdb.InMemoryMetadataStore;
import org.opensearch.tsdb.MetadataStore;
import org.opensearch.tsdb.TSDBPlugin;
import org.opensearch.tsdb.TestUtils;
import org.opensearch.tsdb.core.chunk.Chunk;
import org.opensearch.tsdb.core.chunk.ChunkIterator;
import org.opensearch.tsdb.core.compaction.NoopCompaction;
import org.opensearch.tsdb.core.index.ReaderManagerWithMetadata;
import org.opensearch.tsdb.core.index.closed.ClosedChunk;
import org.opensearch.tsdb.core.index.closed.ClosedChunkIndexIO;
import org.opensearch.tsdb.core.index.closed.ClosedChunkIndexManager;
import org.opensearch.tsdb.core.index.live.LiveSeriesIndex;
import org.opensearch.tsdb.core.model.ByteLabels;
import org.opensearch.tsdb.core.model.Labels;
import org.opensearch.tsdb.core.retention.NOOPRetention;
import org.opensearch.tsdb.core.utils.Constants;
import org.opensearch.tsdb.core.utils.Time;

import java.io.IOException;
import java.nio.file.Path;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

import static org.mockito.Mockito.doReturn;

public class HeadTests extends OpenSearchTestCase {

    private final Settings defaultSettings = Settings.builder()
        .put(TSDBPlugin.TSDB_ENGINE_BLOCK_DURATION.getKey(), TimeValue.timeValueMillis(48000))
        .put(TSDBPlugin.TSDB_ENGINE_CHUNK_DURATION.getKey(), TimeValue.timeValueMillis(8000))
        .put(TSDBPlugin.TSDB_ENGINE_OOO_CUTOFF.getKey(), TimeValue.timeValueMillis(8000))
        .put(TSDBPlugin.TSDB_ENGINE_SAMPLES_PER_CHUNK.getKey(), 120)
        .put(TSDBPlugin.TSDB_ENGINE_TIME_UNIT.getKey(), Constants.Time.DEFAULT_TIME_UNIT.toString())
        .build();

    ThreadPool threadPool;

    public void setUp() throws Exception {
        super.setUp();
        threadPool = new TestThreadPool(
            TSDBPlugin.MGMT_THREAD_POOL_NAME,
            new FixedExecutorBuilder(Settings.builder().build(), TSDBPlugin.MGMT_THREAD_POOL_NAME, 1, 1, "")
        );
    }

    public void tearDown() throws Exception {
        super.tearDown();
        threadPool.shutdownNow();
    }

    public void testHeadLifecycle() throws IOException, InterruptedException {
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("headTest", "headTestUid", 0),
            defaultSettings
        );

        Head head = new Head(
            createTempDir("testHeadLifecycle"),
            new ShardId("headTest", "headTestUid", 0),
            closedChunkIndexManager,
            defaultSettings
        );
        Labels seriesLabels = ByteLabels.fromStrings("k1", "v1", "k2", "v2");

        // create three chunks, with [7, 8, 2] samples respectively
        for (int i = 1; i < 18; i++) {
            long timestamp = i * 1000L;
            double value = i;

            Head.HeadAppender appender = head.newAppender();
            appender.preprocess(Engine.Operation.Origin.PRIMARY, 0, 0, seriesLabels, timestamp, value, () -> {});
            appender.append(() -> {}, () -> {});
        }

        head.getLiveSeriesIndex().getDirectoryReaderManager().maybeRefreshBlocking();

        assertEquals("Initial minTime should be 0", 0L, head.getMinTimestamp());

        head.closeHeadChunks(true);

        assertEquals("minTime should be 8000 after first flush", 8000L, head.getMinTimestamp());

        closedChunkIndexManager.getReaderManagersWithMetadata().forEach(rm -> {
            try {
                rm.readerMananger().maybeRefreshBlocking();
            } catch (IOException e) {
                fail("Failed to refresh ClosedChunkIndexManager ReaderManager: " + e.getMessage());
            }
        });

        // Verify LiveSeriesIndex ReaderManager is accessible
        assertNotNull(head.getLiveSeriesIndex().getDirectoryReaderManager());

        // Verify ClosedChunkIndexManager ReaderManagers are accessible
        List<ReaderManagerWithMetadata> readerManagers = closedChunkIndexManager.getReaderManagersWithMetadata();
        assertFalse(readerManagers.isEmpty());

        List<Object> seriesChunks = getChunks(head, closedChunkIndexManager);
        assertEquals(3, seriesChunks.size());

        assertTrue("First chunk is closed", seriesChunks.get(0) instanceof ClosedChunk);
        assertTrue("Second chunk is still in-memory", seriesChunks.get(1) instanceof MemChunk);
        assertTrue("Third chunk is still in-memory", seriesChunks.get(2) instanceof MemChunk);

        ChunkIterator firstChunk = ((ClosedChunk) seriesChunks.get(0)).getChunkIterator();
        ChunkIterator secondChunk = ((MemChunk) seriesChunks.get(1)).getCompoundChunk().toChunk().iterator();
        ChunkIterator thirdChunk = ((MemChunk) seriesChunks.get(2)).getCompoundChunk().toChunk().iterator();

        TestUtils.assertIteratorEquals(
            firstChunk,
            List.of(1000L, 2000L, 3000L, 4000L, 5000L, 6000L, 7000L),
            List.of(1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0)
        );

        TestUtils.assertIteratorEquals(
            secondChunk,
            List.of(8000L, 9000L, 10000L, 11000L, 12000L, 13000L, 14000L, 15000L),
            List.of(8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0)
        );

        TestUtils.assertIteratorEquals(thirdChunk, List.of(16000L, 17000L), List.of(16.0, 17.0));

        head.close();
        closedChunkIndexManager.close();
    }

    public void testHeadSeriesCleanup() throws IOException, InterruptedException {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("metricsStore");
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(metricsPath, shardId, closedChunkIndexManager, defaultSettings);

        Labels seriesNoData = ByteLabels.fromStrings("k1", "v1", "k2", "v2");
        Labels seriesWithData = ByteLabels.fromStrings("k1", "v1", "k3", "v3");

        head.getOrCreateSeries(seriesNoData.stableHash(), seriesNoData, 0L);
        assertEquals("One series in the series map", 1, head.getSeriesMap().size());
        assertEquals("getNumSeries returns 1", 1, head.getNumSeries());
        for (int i = 1; i < 18; i++) {
            Head.HeadAppender appender = head.newAppender();
            appender.preprocess(
                Engine.Operation.Origin.PRIMARY,
                i,
                seriesWithData.stableHash(),
                seriesWithData,
                i * 1000L,
                i * 10.0,
                () -> {}
            );
            appender.append(() -> {}, () -> {});
        }

        assertEquals("getNumSeries returns 2", 2, head.getNumSeries());
        assertNotNull("Series with last append at seqNo 0 exists", head.getSeriesMap().getByReference(seriesNoData.stableHash()));
        assertNotNull("Series with last append at seqNo 10 exists", head.getSeriesMap().getByReference(seriesWithData.stableHash()));

        // Two chunks were created, minSeqNo of all in-memory chunks is >0 but <9
        head.closeHeadChunks(true);
        assertNull("Series with last append at seqNo 0 is removed", head.getSeriesMap().getByReference(seriesNoData.stableHash()));
        assertNotNull("Series with last append at seqNo 9 exists", head.getSeriesMap().getByReference(seriesWithData.stableHash()));
        assertEquals("One series remain in the series map", 1, head.getSeriesMap().size());
        assertEquals("getNumSeries returns 1", 1, head.getNumSeries());

        // Simulate advancing the time, so the series with data may have it's last chunk closed
        head.updateMaxSeenTimestamp(40000L); // last chunk has range 16000-24000, this should ensure maxTime - oooCutoff is beyond that
        assertEquals(Long.MAX_VALUE, head.closeHeadChunks(true));

        head.close();
        closedChunkIndexManager.close();
    }

    public void testHeadRecovery() throws IOException, InterruptedException {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("metricsStore");
        MetadataStore metadataStore = new InMemoryMetadataStore();
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            metadataStore,
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Path headPath = createTempDir("testHeadRecovery");

        Head head = new Head(headPath, new ShardId("headTest", "headTestUid", 0), closedChunkIndexManager, defaultSettings);
        Labels series1 = ByteLabels.fromStrings("k1", "v1", "k2", "v2");
        long series1Reference = 1L;
        long numSamples = 18;

        for (int i = 0; i < numSamples; i++) {
            Head.HeadAppender appender1 = head.newAppender();
            appender1.preprocess(Engine.Operation.Origin.PRIMARY, i, series1Reference, series1, 1000L * (i + 1), 10 * i, () -> {});
            appender1.append(() -> {}, () -> {});
        }

        long minSeqNo = head.closeHeadChunks(true);
        assertEquals("7 samples were MMAPed, replay from minSeqNo + 1", 6, minSeqNo);
        head.close();
        closedChunkIndexManager.close();

        ClosedChunkIndexManager newClosedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            metadataStore,
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head newHead = new Head(headPath, new ShardId("headTest", "headTestUid", 0), newClosedChunkIndexManager, defaultSettings);

        // MemSeries are correctly loaded and updated from commit data
        assertEquals(series1, newHead.getSeriesMap().getByReference(series1Reference).getLabels());
        assertEquals(8000, newHead.getSeriesMap().getByReference(series1Reference).getMaxMMapTimestamp());
        assertEquals(17, newHead.getSeriesMap().getByReference(series1Reference).getMaxSeqNo());

        // The translog replay correctly skips MMAPed samples
        int i = 0;
        while (i < 7) {
            Head.HeadAppender appender1 = newHead.newAppender();
            appender1.preprocess(Engine.Operation.Origin.PRIMARY, i, series1Reference, series1, 1000L * (i + 1), 10 * i, () -> {});
            assertFalse("Previously MMAPed sample for seqNo " + i + " is not appended again", appender1.append(() -> {}, () -> {}));
            i++;
        }

        // non MMAPed samples are appended
        while (i < numSamples) {
            Head.HeadAppender appender1 = newHead.newAppender();
            appender1.preprocess(Engine.Operation.Origin.PRIMARY, i, series1Reference, series1, 1000L * (i + 1), 10 * i, () -> {});
            assertTrue("Previously in-memory sample for seqNo " + i + " is appended", appender1.append(() -> {}, () -> {}));
            i++;
        }

        newHead.close();
        newClosedChunkIndexManager.close();
    }

    public void testHeadRecoveryWithFailedChunks() throws IOException, InterruptedException {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("metricsStore");
        MetadataStore metadataStore = new InMemoryMetadataStore();

        ClosedChunkIndexManager closedChunkIndexManager = Mockito.spy(
            new ClosedChunkIndexManager(
                metricsPath,
                metadataStore,
                new NOOPRetention(),
                new NoopCompaction(),
                threadPool,
                shardId,
                defaultSettings
            )
        );
        Path headPath = createTempDir("testHeadRecoveryWithCompaction");
        Head head = new Head(headPath, shardId, closedChunkIndexManager, defaultSettings);

        // Create series with multiple closable chunks
        Labels seriesLabels = ByteLabels.fromStrings("k1", "v1", "k2", "v2");
        long seriesRef = 1L;
        long seqNo = 100; // Start at 100 to make tracking easier

        // Create 5 chunks: [0-8000], [8000-16000], [16000-24000], [24000-32000], [32000-40000]
        // Each chunk will have samples with incrementing seqNos
        // 5 chunks are chosen to produce 4 closable chunks, which is an even number to ensure closed chunk selection logic using
        // .addFirst doesn't have the same behavior as .add(last) for testing, and will be caught be mistakenly changed
        for (int i = 0; i < 40; i++) {
            Head.HeadAppender appender = head.newAppender();
            appender.preprocess(Engine.Operation.Origin.PRIMARY, seqNo++, seriesRef, seriesLabels, 1000L * (i + 1), 10.0 * i, () -> {});
            appender.append(() -> {}, () -> {});
        }

        MemSeries series = head.getSeriesMap().getByReference(seriesRef);

        // With maxTime=40000, cutoff=40000-8000-0=32000
        // Closable chunks (oldest to newest):
        // index 0: [0-8000] (seqNo 100-106, minSeqNo=100)
        // index 1: [8000-16000] (seqNo 107-114, minSeqNo=107)
        // index 2: [16000-24000] (seqNo 115-122, minSeqNo=115)
        // index 3: [24000-32000] (seqNo 123-130, minSeqNo=123)
        // Non-closable: [32000-40000], [40000-48000]

        // mock to fail on the 3rd closable chunk (index 2), 1st and 2nd succeed (indices 0 and 1)
        MemSeries.ClosableChunkResult result = series.getClosableChunks(32000L);
        doReturn(false).when(closedChunkIndexManager).addMemChunk(series, result.closableChunks().get(2));

        // closeHeadChunks should return minSeqNo of the first failed chunk minus 1
        // First failed chunk is at index 2: [16000-24000] with minSeqNo 115
        long minSeqNo = head.closeHeadChunks(true);
        assertEquals(114, minSeqNo); // 115 - 1

        head.close();
        closedChunkIndexManager.close();
    }

    public void testHeadRecoveryWithFirstChunkFailing() throws IOException, InterruptedException {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("metricsStore");
        MetadataStore metadataStore = new InMemoryMetadataStore();

        ClosedChunkIndexManager closedChunkIndexManager = Mockito.spy(
            new ClosedChunkIndexManager(
                metricsPath,
                metadataStore,
                new NOOPRetention(),
                new NoopCompaction(),
                threadPool,
                shardId,
                defaultSettings
            )
        );
        Path headPath = createTempDir("testHeadRecoveryWithFirstChunkFailing");
        Head head = new Head(headPath, shardId, closedChunkIndexManager, defaultSettings);

        Labels seriesLabels = ByteLabels.fromStrings("k1", "v1", "k2", "v2");
        long seriesRef = 1L;
        long seqNo = 100;

        // Create 3 chunks: [0-8000], [8000-16000], [16000-24000]
        for (int i = 0; i < 24; i++) {
            Head.HeadAppender appender = head.newAppender();
            appender.preprocess(Engine.Operation.Origin.PRIMARY, seqNo++, seriesRef, seriesLabels, 1000L * (i + 1), 10.0 * i, () -> {});
            appender.append(() -> {}, () -> {});
        }

        MemSeries series = head.getSeriesMap().getByReference(seriesRef);

        // mock to fail on the first closable chunk (index 0)
        MemSeries.ClosableChunkResult result = series.getClosableChunks(16000L);
        doReturn(false).when(closedChunkIndexManager).addMemChunk(series, result.closableChunks().getFirst());

        // closeHeadChunks should handle the failure gracefully and return the first failed chunk's minSeqNo - 1
        long minSeqNo = head.closeHeadChunks(true);
        assertEquals(99, minSeqNo); // 100 - 1

        head.close();
        closedChunkIndexManager.close();
    }

    public void testHeadMinTime() throws IOException, InterruptedException {
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("headTest", "headTestUid", 0),
            defaultSettings
        );

        Head head = new Head(
            createTempDir("testHeadMinTime"),
            new ShardId("headTest", "headTestUid", 0),
            closedChunkIndexManager,
            defaultSettings
        );
        Labels seriesLabels = ByteLabels.fromStrings("k1", "v1", "k2", "v2");

        for (int i = 16; i < 18; i++) {
            long timestamp = i * 1000L;
            double value = i;

            Head.HeadAppender appender = head.newAppender();
            appender.preprocess(Engine.Operation.Origin.PRIMARY, 0, 0, seriesLabels, timestamp, value, () -> {});
            appender.append(() -> {}, () -> {});
        }

        head.getLiveSeriesIndex().getDirectoryReaderManager().maybeRefreshBlocking();

        assertEquals("Initial minTime should be 0", 0L, head.getMinTimestamp());

        head.closeHeadChunks(true);

        // test when minTimestamp = maxTimestamp - oooCutoff
        assertEquals("minTime should be 9000 after first flush", 9000L, head.getMinTimestamp());

        // ingest at 10, 15, 20, 25
        for (int i = 10; i < 26; i += 5) {
            long timestamp = i * 1000L;
            double value = i;

            Head.HeadAppender appender = head.newAppender();
            appender.preprocess(Engine.Operation.Origin.PRIMARY, 0, 0, seriesLabels, timestamp, value, () -> {});
            appender.append(() -> {}, () -> {});
        }

        head.closeHeadChunks(true);

        // test when minTimestamp = openChunk.getMinTimestamp (not equal to any sample's timestamp)
        assertEquals("minTime should be 8000 after second flush", 16000L, head.getMinTimestamp());

        head.close();
        closedChunkIndexManager.close();
    }

    public void testHeadCloseHeadChunksEmpty() throws IOException {
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("headTest", "headTestUid", 0),
            defaultSettings
        );

        Head head = new Head(
            createTempDir("testHeadCloseHeadChunksEmpty"),
            new ShardId("headTest", "headTestUid", 0),
            closedChunkIndexManager,
            defaultSettings
        );
        head.getLiveSeriesIndex().getDirectoryReaderManager().maybeRefreshBlocking();
        assertEquals("Initial minTime should be 0", 0L, head.getMinTimestamp());

        head.closeHeadChunks(true);
        assertEquals("After flush minTime should remain 0", 0L, head.getMinTimestamp());

        head.close();
        closedChunkIndexManager.close();
    }

    public void testHeadGetOrCreateSeries() throws IOException {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("testGetOrCreateSeries");
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(metricsPath, shardId, closedChunkIndexManager, defaultSettings);
        Labels labels = ByteLabels.fromStrings("k1", "v1", "k2", "v2");

        // Test creating a new series with specified hash
        Head.SeriesResult result1 = head.getOrCreateSeries(123L, labels, 100L);
        assertTrue(result1.created());
        assertNotNull(result1.series());
        assertEquals(123L, result1.series().getReference());
        assertEquals(labels, result1.series().getLabels());

        // Test created the same series again - should return existing rather than creating
        Head.SeriesResult result2 = head.getOrCreateSeries(123L, labels, 200L);
        assertFalse(result2.created());
        assertEquals(result1.series(), result2.series());
        assertEquals(123L, result2.series().getReference());

        head.close();
        closedChunkIndexManager.close();
    }

    public void testHeadGetOrCreateSeriesHandlesHashFunctionChange() throws IOException {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("testGetOrCreateSeries");
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(metricsPath, shardId, closedChunkIndexManager, defaultSettings);
        Labels labels = ByteLabels.fromStrings("k1", "v1", "k2", "v2");

        // Test creating a new series with specified hash
        Head.SeriesResult result1 = head.getOrCreateSeries(123L, labels, 100L);
        assertTrue(result1.created());
        assertNotNull(result1.series());
        assertEquals(123L, result1.series().getReference());
        assertEquals(labels, result1.series().getLabels());

        // Test created the same series again, but with another hash. Should result in two distinct series
        Head.SeriesResult result2 = head.getOrCreateSeries(labels.stableHash(), labels, 200L);
        assertTrue(result2.created());
        assertNotEquals(result1.series(), result2.series());
        assertEquals(labels.stableHash(), result2.series().getReference());

        head.close();
        closedChunkIndexManager.close();
    }

    public void testGetOrCreateSeriesConcurrent() throws Exception {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("testGetOrCreateSeriesConcurrent");
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(metricsPath, shardId, closedChunkIndexManager, defaultSettings);
        long hash = 123L;

        // these values for # threads and iterations were chosen to reliably cause contention, based on code coverage inspection
        int numThreads = 50;
        int iterations = 100;

        for (int iter = 0; iter < iterations; iter++) {
            long currentHash = hash + iter;
            Labels currentLabels = ByteLabels.fromStrings("k1", "v1", "iteration", String.valueOf(iter));

            Head.SeriesResult[] results = new Head.SeriesResult[numThreads];
            Thread[] threads = new Thread[numThreads];
            CountDownLatch startLatch = new CountDownLatch(1);

            for (int i = 0; i < numThreads; i++) {
                int threadId = i;
                threads[i] = new Thread(() -> {
                    try {
                        startLatch.await();
                        results[threadId] = head.getOrCreateSeries(currentHash, currentLabels, 1000L + threadId);
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                    }
                });
            }

            for (Thread thread : threads) {
                thread.start();
            }

            startLatch.countDown(); // attempt to start all threads at the same time, to ensure contention

            for (Thread thread : threads) {
                thread.join(5000);
            }

            assertEquals(
                "Only one series should exist for each hash",
                1,
                head.getSeriesMap().getSeriesMap().stream().mapToLong(MemSeries::getReference).filter(ref -> ref == currentHash).count()
            );

            MemSeries actualSeries = head.getSeriesMap().getByReference(currentHash);
            assertNotNull("Series should exist in map for each hash", actualSeries);

            int createdCount = 0;
            for (Head.SeriesResult result : results) {
                assertEquals("All threads should get the same series instance", actualSeries, result.series());
                if (result.created()) {
                    createdCount++;
                }
            }

            assertEquals("Exactly one thread should report creation", 1, createdCount);
        }

        head.close();
        closedChunkIndexManager.close();
    }

    /**
     * Test preprocess with concurrent threads and a single liveSeriesIndex.addSeries failure.
     * This simulates a race condition in seriesMap.putIfAbsent where a failed series might be deleted
     * from the map while another thread is trying to create it.
     */
    @SuppressForbidden(reason = "reflection usage is required here")
    public void testPreprocessConcurrentWithAddSeriesFailure() throws Exception {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("testPreprocessConcurrentWithAddSeriesFailure");
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );

        // Create a spy on LiveSeriesIndex to inject failures
        Head head = new Head(metricsPath, shardId, closedChunkIndexManager, defaultSettings);
        LiveSeriesIndex liveSeriesIndexSpy = Mockito.spy(head.getLiveSeriesIndex());

        // Use reflection to replace the liveSeriesIndex with the spy
        java.lang.reflect.Field liveSeriesIndexField = Head.class.getDeclaredField("liveSeriesIndex");
        liveSeriesIndexField.setAccessible(true);
        liveSeriesIndexField.set(head, liveSeriesIndexSpy);

        int numThreads = 50;
        int iterations = 100;

        for (int iter = 0; iter < iterations; iter++) {
            final int currentIter = iter;
            Labels currentLabels = ByteLabels.fromStrings("k1", "v1", "iteration", String.valueOf(currentIter));
            long currentHash = currentLabels.stableHash();

            // Introduce a single failure in addSeries for each iteration
            AtomicInteger callCount = new AtomicInteger(0);
            Mockito.doAnswer(invocation -> {
                if (callCount.incrementAndGet() == 1) {
                    // First call fails
                    throw new RuntimeException("Simulated addSeries failure");
                }
                // Subsequent calls succeed
                invocation.callRealMethod();
                return null;
            }).when(liveSeriesIndexSpy).addSeries(Mockito.any(), Mockito.anyLong(), Mockito.anyLong());

            Boolean[] results = new Boolean[numThreads];
            Exception[] exceptions = new Exception[numThreads];
            Thread[] threads = new Thread[numThreads];
            CountDownLatch startLatch = new CountDownLatch(1);

            for (int i = 0; i < numThreads; i++) {
                int threadId = i;
                threads[i] = new Thread(() -> {
                    try {
                        startLatch.await();
                        Head.HeadAppender appender = head.newAppender();
                        results[threadId] = appender.preprocess(
                            Engine.Operation.Origin.PRIMARY,
                            currentIter * numThreads + threadId,
                            currentHash,
                            currentLabels,
                            1000L + threadId,
                            100.0 + threadId,
                            () -> {}
                        );
                    } catch (Exception e) {
                        exceptions[threadId] = e;
                    }
                });
            }

            for (Thread thread : threads) {
                thread.start();
            }

            startLatch.countDown(); // Start all threads at the same time

            for (Thread thread : threads) {
                thread.join(5000);
            }

            // There are only 2 possible scenarios:
            // 1. A new series is created after the initial one is deleted (1 thread returns true from preprocess)
            // 2. No new series is created (0 threads return true from preprocess)

            // Check 1: Exactly 1 exception must be thrown (from the thread that hit addSeries failure)
            int exceptionCount = 0;
            for (int i = 0; i < numThreads; i++) {
                if (exceptions[i] != null) {
                    exceptionCount++;
                }
            }
            assertEquals("Exactly one thread must throw exception for iteration " + currentIter, 1, exceptionCount);

            // Check 2: Count how many threads returned true from preprocess (created series)
            int createdCount = 0;
            for (int i = 0; i < numThreads; i++) {
                if (results[i] != null && results[i]) {
                    createdCount++;
                }
            }
            assertTrue(
                "Either 0 or 1 thread should return true from preprocess for iteration " + currentIter,
                createdCount == 0 || createdCount == 1
            );

            // Check 3: If a thread returned true, verify series exists and is not marked failed
            if (createdCount == 1) {
                MemSeries series = head.getSeriesMap().getByReference(currentHash);
                assertNotNull("Series must exist if a thread returned true from preprocess for iteration " + currentIter, series);
                assertFalse("Series must not be marked as failed for iteration " + currentIter, series.isFailed());
            } else {
                assertNull(head.getSeriesMap().getByReference(currentHash));
            }
        }

        head.close();
        closedChunkIndexManager.close();
    }

    /**
     * Test that markSeriesAsFailed removes the series from the LiveSeriesIndex.
     */
    public void testMarkSeriesAsFailed() throws Exception {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("testMarkSeriesAsFailed");
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );

        Head head = new Head(metricsPath, shardId, closedChunkIndexManager, defaultSettings);

        // Preprocess a new series successfully
        Labels labels = ByteLabels.fromStrings("k1", "v1", "k2", "v2");
        long hash = labels.stableHash();
        Head.HeadAppender appender = head.newAppender();
        boolean created = appender.preprocess(Engine.Operation.Origin.PRIMARY, 0, hash, labels, 1000L, 100.0, () -> {});

        assertTrue("Series should be created", created);
        MemSeries series = head.getSeriesMap().getByReference(hash);
        assertNotNull("Series should exist in seriesMap", series);

        // Refresh the reader to see the newly added series
        head.getLiveSeriesIndex().getDirectoryReaderManager().maybeRefresh();

        // Verify series exists in LiveSeriesIndex by counting documents
        DirectoryReader readerBefore = head.getLiveSeriesIndex().getDirectoryReaderManager().acquire();
        try {
            int seriesCountBefore = readerBefore.numDocs();
            assertEquals("Should have 1 series in live index", 1, seriesCountBefore);
        } finally {
            head.getLiveSeriesIndex().getDirectoryReaderManager().release(readerBefore);
        }

        // Mark series as failed
        head.markSeriesAsFailed(series);

        // Verify series is marked as failed
        assertTrue("Series should be marked as failed", series.isFailed());

        // Verify series is deleted from seriesMap
        assertNull("Series should be removed from seriesMap", head.getSeriesMap().getByReference(hash));

        // Refresh the reader to see the deletion
        head.getLiveSeriesIndex().getDirectoryReaderManager().maybeRefresh();

        // Verify series is deleted from LiveSeriesIndex by counting documents
        DirectoryReader readerAfter = head.getLiveSeriesIndex().getDirectoryReaderManager().acquire();
        try {
            int seriesCountAfter = readerAfter.numDocs();
            assertEquals("Should have 0 series in live index after marking as failed", 0, seriesCountAfter);
        } finally {
            head.getLiveSeriesIndex().getDirectoryReaderManager().release(readerAfter);
        }

        head.close();
        closedChunkIndexManager.close();
    }

    // Utility method to return all chunks from both LiveSeriesIndex and ClosedChunkIndexes
    private List<Object> getChunks(Head head, ClosedChunkIndexManager closedChunkIndexManager) throws IOException {
        List<Object> chunks = new ArrayList<>();

        // Query ClosedChunkIndexes
        List<ReaderManagerWithMetadata> closedReaderManagers = closedChunkIndexManager.getReaderManagersWithMetadata();
        for (int i = 0; i < closedReaderManagers.size(); i++) {
            ReaderManager closedReaderManager = closedReaderManagers.get(i).readerMananger();
            DirectoryReader closedReader = null;
            try {
                closedReader = closedReaderManager.acquire();
                IndexSearcher closedSearcher = new IndexSearcher(closedReader);
                TopDocs topDocs = closedSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);

                for (LeafReaderContext leaf : closedReader.leaves()) {
                    BinaryDocValues docValues = leaf.reader()
                        .getBinaryDocValues(org.opensearch.tsdb.core.mapping.Constants.IndexSchema.CHUNK);
                    if (docValues == null) {
                        continue;
                    }
                    int docBase = leaf.docBase;
                    for (ScoreDoc sd : topDocs.scoreDocs) {
                        int docId = sd.doc;
                        if (docId >= docBase && docId < docBase + leaf.reader().maxDoc()) {
                            int localDocId = docId - docBase;
                            if (docValues.advanceExact(localDocId)) {
                                BytesRef ref = docValues.binaryValue();
                                ClosedChunk chunk = ClosedChunkIndexIO.getClosedChunkFromSerialized(ref);
                                chunks.add(chunk);
                            }
                        }
                    }
                }
            } finally {
                if (closedReader != null) {
                    closedReaderManager.release(closedReader);
                }
            }
        }

        List<MemChunk> liveChunks = new ArrayList<>();
        // Query LiveSeriesIndex
        ReaderManager liveReaderManager = head.getLiveSeriesIndex().getDirectoryReaderManager();
        DirectoryReader liveReader = null;
        try {
            liveReader = liveReaderManager.acquire();
            IndexSearcher liveSearcher = new IndexSearcher(liveReader);
            TopDocs topDocs = liveSearcher.search(new MatchAllDocsQuery(), Integer.MAX_VALUE);

            for (LeafReaderContext leaf : liveReader.leaves()) {
                NumericDocValues docValues = leaf.reader()
                    .getNumericDocValues(org.opensearch.tsdb.core.mapping.Constants.IndexSchema.REFERENCE);
                if (docValues == null) {
                    continue;
                }
                int docBase = leaf.docBase;
                for (ScoreDoc sd : topDocs.scoreDocs) {
                    int docId = sd.doc;
                    if (docId >= docBase && docId < docBase + leaf.reader().maxDoc()) {
                        int localDocId = docId - docBase;
                        if (docValues.advanceExact(localDocId)) {
                            long ref = docValues.longValue();
                            MemSeries series = head.getSeriesMap().getByReference(ref);
                            MemChunk chunk = series.getHeadChunk();
                            while (chunk != null) {
                                liveChunks.add(chunk);
                                chunk = chunk.getPrev();
                            }
                        }
                    }
                }
            }
        } finally {
            if (liveReader != null) {
                liveReaderManager.release(liveReader);
            }
        }

        liveChunks.sort(Comparator.comparingLong(MemChunk::getMaxTimestamp));
        chunks.addAll(liveChunks);
        return chunks;
    }

    // Utility method to append all samples from a Chunk to lists
    private void appendChunk(Chunk chunk, List<Long> timestamps, List<Double> values) {
        appendIterator(chunk.iterator(), timestamps, values);
    }

    // Utility method to append all samples from a ChunkIterator to lists
    private void appendIterator(ChunkIterator iterator, List<Long> timestamps, List<Double> values) {
        while (iterator.next() != ChunkIterator.ValueType.NONE) {
            ChunkIterator.TimestampValue tv = iterator.at();
            timestamps.add(tv.timestamp());
            values.add(tv.value());
        }
    }

    public void testNewAppender() throws IOException {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("testNewAppender");
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(metricsPath, shardId, closedChunkIndexManager, defaultSettings);

        // Test that newAppender returns non-null
        Head.HeadAppender appender1 = head.newAppender();
        assertNotNull("newAppender should return non-null instance", appender1);

        // Test that newAppender returns different instances
        Head.HeadAppender appender2 = head.newAppender();
        assertNotNull("second newAppender call should return non-null instance", appender2);
        assertNotSame("newAppender should return different instances", appender1, appender2);

        head.close();
        closedChunkIndexManager.close();
    }

    public void testSeriesCreatorThreadExecutesRunnableFirst() throws Exception {
        ShardId shardId = new ShardId("headTest", "headTestUid", 0);
        Path metricsPath = createTempDir("testSeriesCreatorThread");
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            metricsPath,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(metricsPath, shardId, closedChunkIndexManager, defaultSettings);

        // Use high thread count and iterations to reliably induce contention
        int numThreads = 50;
        int iterations = 100;

        for (int iter = 0; iter < iterations; iter++) {
            Labels labels = ByteLabels.fromStrings("k1", "v1", "iteration", String.valueOf(iter));
            long hash = labels.stableHash();

            CountDownLatch startLatch = new CountDownLatch(1);
            List<Boolean> createdResults = Collections.synchronizedList(new ArrayList<>());

            Thread[] threads = new Thread[numThreads];

            for (int i = 0; i < numThreads; i++) {
                int threadId = i;
                threads[i] = new Thread(() -> {
                    try {
                        startLatch.await();
                        Head.HeadAppender appender = head.newAppender();
                        boolean created = appender.preprocess(
                            Engine.Operation.Origin.PRIMARY,
                            threadId,
                            hash,
                            labels,
                            100L + threadId,
                            1.0,
                            () -> {}
                        );
                        appender.append(() -> createdResults.add(created), () -> {});
                    } catch (InterruptedException e) {
                        fail("Thread was interrupted: " + e.getMessage());
                    }
                });
            }

            for (Thread thread : threads) {
                thread.start();
            }

            startLatch.countDown(); // Start all threads simultaneously to ensure contention

            for (Thread thread : threads) {
                thread.join(5000);
            }

            assertTrue("First appender should create the series", createdResults.getFirst());
            for (int i = 1; i < numThreads; i++) {
                assertFalse("Subsequent appenders should not create the series", createdResults.get(i));
            }

            // Verify only one series was created
            MemSeries series = head.getSeriesMap().getByReference(hash);
            assertNotNull("Series should exist", series);
            assertEquals("One series created per iteration", iter + 1, head.getNumSeries());
        }

        head.close();
        closedChunkIndexManager.close();
    }

    public void testPreprocessFailure() throws IOException {
        Path tempDir = createTempDir();
        ShardId shardId = new ShardId("test", "test", 0);
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            tempDir,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(tempDir, shardId, closedChunkIndexManager, defaultSettings);

        // Pass empty labels to trigger failure in preprocess
        Labels emptyLabels = ByteLabels.fromStrings(); // Empty labels
        long hash = 12345L;

        Head.HeadAppender appender = head.newAppender();

        // Expect exception for empty labels
        TSDBEmptyLabelException ex = assertThrows(
            TSDBEmptyLabelException.class,
            () -> appender.preprocess(Engine.Operation.Origin.PRIMARY, 0, hash, emptyLabels, 2000L, 100.0, () -> {})
        );
        assertTrue(ex.getMessage().contains("Labels"));

        head.close();
        closedChunkIndexManager.close();
    }

    public void testPreprocessFailureDeletesSeries() throws IOException {
        Path tempDir = createTempDir();
        ShardId shardId = new ShardId("test", "test", 0);
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            tempDir,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(tempDir, shardId, closedChunkIndexManager, defaultSettings);

        // Close the live series index to trigger exception during series creation
        head.getLiveSeriesIndex().close();

        Labels labels = ByteLabels.fromStrings("__name__", "test_metric", "host", "server1");
        long hash = labels.stableHash();

        Head.HeadAppender appender = head.newAppender();
        boolean[] failureCallbackCalled = { false };

        // Preprocess should fail when trying to add series to closed index
        // This will throw a TSDBTragicException (or RuntimeException wrapping it)
        assertThrows(
            RuntimeException.class,
            () -> appender.preprocess(Engine.Operation.Origin.PRIMARY, 0, hash, labels, 2000L, 100.0, () -> failureCallbackCalled[0] = true)
        );

        assertFalse("Failure callback should not be called for tragic exceptions", failureCallbackCalled[0]);

        MemSeries series = head.getSeriesMap().getByReference(hash);
        assertNull("Series should be deleted on failure", series);

        closedChunkIndexManager.close();
    }

    public void testAppendWithNullSeries() throws IOException, InterruptedException {
        Path tempDir = createTempDir();
        ShardId shardId = new ShardId("test", "test", 0);
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            tempDir,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(tempDir, shardId, closedChunkIndexManager, defaultSettings);

        // Create appender but don't call preprocess (series will be null)
        Head.HeadAppender appender = head.newAppender();

        boolean[] successCalled = { false };
        boolean[] failureCalled = { false };

        // Call append without preprocess - should detect null series and call failureCallback
        assertThrows(RuntimeException.class, () -> appender.append(() -> successCalled[0] = true, () -> failureCalled[0] = true));

        assertFalse("Success callback should not be called", successCalled[0]);
        assertTrue("Failure callback should be called for null series", failureCalled[0]);

        head.close();
        closedChunkIndexManager.close();
    }

    public void testTranslogWriteFailure() throws IOException, InterruptedException {
        Path tempDir = createTempDir();
        ShardId shardId = new ShardId("test", "test", 0);
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            tempDir,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(tempDir, shardId, closedChunkIndexManager, defaultSettings);

        // Create series successfully
        Labels labels = ByteLabels.fromStrings("__name__", "test_metric", "host", "server1");
        long hash = labels.stableHash();

        Head.HeadAppender appender = head.newAppender();
        boolean created = appender.preprocess(Engine.Operation.Origin.PRIMARY, 0, hash, labels, 2000L, 100.0, () -> {});
        assertTrue("Should create series", created);

        // Simulate callback failure - success callback throws exception
        boolean[] failureCalled = { false };
        boolean[] successCalled = { false };

        MemSeries series = head.getSeriesMap().getByReference(hash);
        RuntimeException ex = assertThrows(RuntimeException.class, () -> appender.append(() -> {
            successCalled[0] = true;
            throw new RuntimeException("Simulated callback failure");
        }, () -> failureCalled[0] = true));
        assertTrue("Exception should mention callback failure", ex.getMessage().contains("Simulated callback failure"));

        assertTrue("Success callback should be called before throwing", successCalled[0]);
        assertTrue("Failure callback should be called when callback fails", failureCalled[0]);

        assertTrue("Series should be marked as failed after callback exception", series.isFailed());
        assertNull("Series should not exist", head.getSeriesMap().getByReference(hash));

        head.close();
        closedChunkIndexManager.close();
    }

    public void testAppendDetectsFailedSeries() throws IOException, InterruptedException {
        Path tempDir = createTempDir();
        ShardId shardId = new ShardId("test", "test", 0);
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            tempDir,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(tempDir, shardId, closedChunkIndexManager, defaultSettings);

        Labels labels = ByteLabels.fromStrings("__name__", "test_metric", "host", "server1");
        long hash = labels.stableHash();

        // 1. Successfully preprocess and append first sample
        Head.HeadAppender appender1 = head.newAppender();
        boolean created1 = appender1.preprocess(Engine.Operation.Origin.PRIMARY, 0, hash, labels, 2000L, 100.0, () -> {});
        assertTrue("First appender should create series", created1);
        appender1.append(() -> {}, () -> {});

        // 2. For same series, preprocess next sample
        Head.HeadAppender appender2 = head.newAppender();
        boolean created2 = appender2.preprocess(Engine.Operation.Origin.PRIMARY, 1, hash, labels, 3000L, 200.0, () -> {});
        assertFalse("Second appender should not create series", created2);

        // 3. Before append, set series to failed
        MemSeries series = head.getSeriesMap().getByReference(hash);
        series.markFailed();

        // 4. Call append and expect failure callback to be called
        boolean[] successCalled = { false };
        boolean[] failureCalled = { false };

        assertThrows(RuntimeException.class, () -> appender2.append(() -> successCalled[0] = true, () -> failureCalled[0] = true));

        assertFalse("Success callback should not be called for failed series", successCalled[0]);
        assertTrue("Failure callback should be called for failed series", failureCalled[0]);

        head.close();
        closedChunkIndexManager.close();
    }

    /**
     * Test that a failed series gets replaced with a new series on retry.
     */
    public void testFailedSeriesGetsReplacedOnRetry() throws IOException, InterruptedException {
        Path tempDir = createTempDir();
        ShardId shardId = new ShardId("test", "test", 0);
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            tempDir,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(tempDir, shardId, closedChunkIndexManager, defaultSettings);

        Labels labels = ByteLabels.fromStrings("__name__", "test_metric", "host", "server1");
        long hash = labels.stableHash();

        // First attempt - create series and mark it as failed
        Head.HeadAppender appender1 = head.newAppender();
        appender1.preprocess(Engine.Operation.Origin.PRIMARY, 0, hash, labels, 1000L, 100.0, () -> {});

        MemSeries series1 = head.getSeriesMap().getByReference(hash);
        assertNotNull("First series should be created", series1);
        head.markSeriesAsFailed(series1);

        // Second attempt - should create a new series replacing the failed one
        Head.HeadAppender appender2 = head.newAppender();
        boolean created = appender2.preprocess(Engine.Operation.Origin.PRIMARY, 1, hash, labels, 2000L, 200.0, () -> {});

        assertTrue("Second preprocess should create a new series", created);

        MemSeries series2 = head.getSeriesMap().getByReference(hash);
        assertNotNull("Series should exist after retry", series2);
        assertFalse("New series should not be marked as failed", series2.isFailed());
        assertNotSame("Should be a different series instance", series1, series2);

        head.close();
        closedChunkIndexManager.close();
    }

    /**
     * Test that OOO validation is enforced for PRIMARY origin.
     */
    public void testOOOValidationWithPrimaryOrigin() throws IOException, InterruptedException {
        Path tempDir = createTempDir();
        ShardId shardId = new ShardId("test", "test", 0);
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            tempDir,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(tempDir, shardId, closedChunkIndexManager, defaultSettings);

        Labels labels = ByteLabels.fromStrings("__name__", "test_metric", "host", "server1");
        long hash = labels.stableHash();

        // First sample to establish maxTime
        Head.HeadAppender appender1 = head.newAppender();
        boolean created1 = appender1.preprocess(Engine.Operation.Origin.PRIMARY, 0, hash, labels, 10000L, 100.0, () -> {});
        assertTrue("First sample should create series", created1);
        appender1.append(() -> {}, () -> {});

        // Sample within OOO window (OOO cutoff = 8000ms, so 10000 - 8000 = 2000). Sample at 5000 is within window.
        Head.HeadAppender appender2 = head.newAppender();
        boolean created2 = appender2.preprocess(Engine.Operation.Origin.PRIMARY, 1, hash, labels, 5000L, 200.0, () -> {});
        assertFalse("Second sample should not create series", created2);
        appender2.append(() -> {}, () -> {});

        // Sample outside OOO window - should throw exception (1000 < 2000)
        Head.HeadAppender appender3 = head.newAppender();
        TSDBOutOfOrderException ex = assertThrows(
            TSDBOutOfOrderException.class,
            () -> appender3.preprocess(Engine.Operation.Origin.PRIMARY, 2, hash, labels, 1000L, 300.0, () -> {})
        );
        assertTrue("Exception message should mention OOO cutoff", ex.getMessage().contains("OOO cutoff"));

        // Sample exactly at cutoff boundary (2000 == 2000) - should succeed
        Head.HeadAppender appender4 = head.newAppender();
        appender4.preprocess(Engine.Operation.Origin.PRIMARY, 3, hash, labels, 2000L, 400.0, () -> {});

        // Sample 1ms before cutoff (1999 < 2000) - should fail
        Head.HeadAppender appender5 = head.newAppender();
        assertThrows(
            TSDBOutOfOrderException.class,
            () -> appender5.preprocess(Engine.Operation.Origin.PRIMARY, 4, hash, labels, 1999L, 500.0, () -> {})
        );

        // Query liveSeriesIndex and validate MIN_TIMESTAMP for the series ref
        long minTimestamp = getMinTimestampFromLiveSeriesIndex(head.getLiveSeriesIndex(), hash);
        TimeUnit timeUnit = TimeUnit.valueOf(TSDBPlugin.TSDB_ENGINE_TIME_UNIT.get(defaultSettings));
        long oooCutoffWindow = Time.toTimestamp(TSDBPlugin.TSDB_ENGINE_OOO_CUTOFF.get(defaultSettings), timeUnit);
        long expectedMinTimestamp = 10000L - oooCutoffWindow;
        assertEquals("MIN_TIMESTAMP should be timestamp - oooCutoffWindow", expectedMinTimestamp, minTimestamp);

        head.close();
        closedChunkIndexManager.close();
    }

    /**
     * Test that OOO validation is bypassed for non-PRIMARY origins.
     */
    public void testOOOValidationBypassedForNonPrimaryOrigins() throws IOException, InterruptedException {
        Path tempDir = createTempDir();
        ShardId shardId = new ShardId("test", "test", 0);
        ClosedChunkIndexManager closedChunkIndexManager = new ClosedChunkIndexManager(
            tempDir,
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            shardId,
            defaultSettings
        );
        Head head = new Head(tempDir, shardId, closedChunkIndexManager, defaultSettings);

        // Test REPLICA origin
        Labels labels1 = ByteLabels.fromStrings("__name__", "metric1", "host", "server1");
        long hash1 = labels1.stableHash();
        Head.HeadAppender appender1 = head.newAppender();
        appender1.preprocess(Engine.Operation.Origin.REPLICA, 0, hash1, labels1, 10000L, 100.0, () -> {});
        appender1.append(() -> {}, () -> {});

        Head.HeadAppender appender2 = head.newAppender();
        appender2.preprocess(Engine.Operation.Origin.REPLICA, 1, hash1, labels1, 1000L, 200.0, () -> {});
        appender2.append(() -> {}, () -> {});

        // Test PEER_RECOVERY origin
        Labels labels2 = ByteLabels.fromStrings("__name__", "metric2", "host", "server2");
        long hash2 = labels2.stableHash();
        Head.HeadAppender appender3 = head.newAppender();
        appender3.preprocess(Engine.Operation.Origin.PEER_RECOVERY, 2, hash2, labels2, 10000L, 100.0, () -> {});
        appender3.append(() -> {}, () -> {});

        Head.HeadAppender appender4 = head.newAppender();
        appender4.preprocess(Engine.Operation.Origin.PEER_RECOVERY, 3, hash2, labels2, 1000L, 200.0, () -> {});
        appender4.append(() -> {}, () -> {});

        // Test LOCAL_TRANSLOG_RECOVERY origin
        Labels labels3 = ByteLabels.fromStrings("__name__", "metric3", "host", "server3");
        long hash3 = labels3.stableHash();
        Head.HeadAppender appender5 = head.newAppender();
        appender5.preprocess(Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY, 4, hash3, labels3, 10000L, 100.0, () -> {});
        appender5.append(() -> {}, () -> {});

        Head.HeadAppender appender6 = head.newAppender();
        appender6.preprocess(Engine.Operation.Origin.LOCAL_TRANSLOG_RECOVERY, 5, hash3, labels3, 1000L, 200.0, () -> {});
        appender6.append(() -> {}, () -> {});

        // Verify all series exist with correct maxSeqNo
        assertEquals(1, head.getSeriesMap().getByReference(hash1).getMaxSeqNo());
        assertEquals(3, head.getSeriesMap().getByReference(hash2).getMaxSeqNo());
        assertEquals(5, head.getSeriesMap().getByReference(hash3).getMaxSeqNo());

        head.close();
        closedChunkIndexManager.close();
    }

    public void testStubSeriesCreationDuringRecovery() throws IOException, InterruptedException {
        ClosedChunkIndexManager cm = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("test", "uuid", 0),
            defaultSettings
        );
        Head head = new Head(createTempDir("head"), new ShardId("test", "uuid", 0), cm, defaultSettings);

        Labels labels = ByteLabels.fromStrings("k1", "v1");
        long ref = labels.stableHash();

        Head.HeadAppender appender = head.newAppender();
        assertTrue(appender.preprocess(Engine.Operation.Origin.PEER_RECOVERY, 0, ref, null, 1000L, 100.0, () -> {}));
        appender.append(() -> {}, () -> {});

        MemSeries series = head.getSeriesMap().getByReference(ref);
        assertTrue(series.isStub());
        assertNull(series.getLabels());

        head.close();
        cm.close();
    }

    public void testStubSeriesUpgradeDuringRecovery() throws IOException, InterruptedException {
        ClosedChunkIndexManager cm = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("test", "uuid", 0),
            defaultSettings
        );
        Head head = new Head(createTempDir("head"), new ShardId("test", "uuid", 0), cm, defaultSettings);

        Labels labels = ByteLabels.fromStrings("k1", "v1");
        long ref = labels.stableHash();

        // Create stub
        Head.HeadAppender a1 = head.newAppender();
        assertTrue(a1.preprocess(Engine.Operation.Origin.PEER_RECOVERY, 0, ref, null, 1000L, 100.0, () -> {}));
        a1.append(() -> {}, () -> {});

        // Upgrade with labels
        Head.HeadAppender a2 = head.newAppender();
        assertTrue(a2.preprocess(Engine.Operation.Origin.PEER_RECOVERY, 1, ref, labels, 2000L, 200.0, () -> {}));
        a2.append(() -> {}, () -> {});

        MemSeries upgraded = head.getSeriesMap().getByReference(ref);
        assertFalse(upgraded.isStub());
        assertEquals(labels, upgraded.getLabels());

        // Query liveSeriesIndex and validate MIN_TIMESTAMP for the series ref
        long minTimestamp = getMinTimestampFromLiveSeriesIndex(head.getLiveSeriesIndex(), ref);
        TimeUnit timeUnit = TimeUnit.valueOf(TSDBPlugin.TSDB_ENGINE_TIME_UNIT.get(defaultSettings));
        long oooCutoffWindow = Time.toTimestamp(TSDBPlugin.TSDB_ENGINE_OOO_CUTOFF.get(defaultSettings), timeUnit);
        long expectedMinTimestamp = 2000L - oooCutoffWindow;
        assertEquals("MIN_TIMESTAMP should be timestamp - oooCutoffWindow", expectedMinTimestamp, minTimestamp);

        head.close();
        cm.close();
    }

    public void testMultipleRefOnlyOperationsBeforeLabels() throws IOException, InterruptedException {
        ClosedChunkIndexManager cm = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("test", "uuid", 0),
            defaultSettings
        );
        Head head = new Head(createTempDir("head"), new ShardId("test", "uuid", 0), cm, defaultSettings);

        Labels labels = ByteLabels.fromStrings("k1", "v1");
        long ref = labels.stableHash();

        for (int i = 0; i < 5; i++) {
            Head.HeadAppender appender = head.newAppender();
            appender.preprocess(Engine.Operation.Origin.PEER_RECOVERY, i, ref, null, 1000L + i * 100, 100.0, () -> {});
            appender.append(() -> {}, () -> {});
        }

        assertTrue(head.getSeriesMap().getByReference(ref).isStub());

        // Upgrade
        Head.HeadAppender appender = head.newAppender();
        appender.preprocess(Engine.Operation.Origin.PEER_RECOVERY, 5, ref, labels, 1500L, 105.0, () -> {});
        appender.append(() -> {}, () -> {});

        MemSeries upgraded = head.getSeriesMap().getByReference(ref);
        assertFalse(upgraded.isStub());

        // Query liveSeriesIndex and validate MIN_TIMESTAMP for the series ref
        long minTimestamp = getMinTimestampFromLiveSeriesIndex(head.getLiveSeriesIndex(), ref);
        TimeUnit timeUnit = TimeUnit.valueOf(TSDBPlugin.TSDB_ENGINE_TIME_UNIT.get(defaultSettings));
        long oooCutoffWindow = Time.toTimestamp(TSDBPlugin.TSDB_ENGINE_OOO_CUTOFF.get(defaultSettings), timeUnit);
        long expectedMinTimestamp = 1500L - oooCutoffWindow;
        assertEquals("MIN_TIMESTAMP should be timestamp - oooCutoffWindow", expectedMinTimestamp, minTimestamp);

        head.close();
        cm.close();
    }

    public void testNonRecoveryCannotCreateStubSeries() throws IOException {
        ClosedChunkIndexManager cm = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("test", "uuid", 0),
            defaultSettings
        );
        Head head = new Head(createTempDir("head"), new ShardId("test", "uuid", 0), cm, defaultSettings);

        Labels labels = ByteLabels.fromStrings("k1", "v1");
        long ref = labels.stableHash();

        Head.HeadAppender appender = head.newAppender();
        assertThrows(
            TSDBEmptyLabelException.class,
            () -> appender.preprocess(Engine.Operation.Origin.PRIMARY, 0, ref, null, 1000L, 100.0, () -> {})
        );

        head.close();
        cm.close();
    }

    public void testFlushSkipsStubSeriesWhenDropNotAllowed() throws IOException, InterruptedException {
        ClosedChunkIndexManager cm = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("test", "uuid", 0),
            defaultSettings
        );
        Head head = new Head(createTempDir("head"), new ShardId("test", "uuid", 0), cm, defaultSettings);

        Labels labels = ByteLabels.fromStrings("k1", "v1");
        long ref = labels.stableHash();

        // Create a stub series with data during recovery
        Head.HeadAppender appender = head.newAppender();
        assertTrue(appender.preprocess(Engine.Operation.Origin.PEER_RECOVERY, 0, ref, null, 1000L, 100.0, () -> {}));
        appender.append(() -> {}, () -> {});

        // Verify series is a stub with data
        MemSeries series = head.getSeriesMap().getByReference(ref);
        assertTrue("Series should be stub", series.isStub());
        assertNull("Stub series should have null labels", series.getLabels());
        assertEquals("Stub counter should be 1", 1, head.getSeriesMap().getStubSeriesCount());

        // Flush with allowDropEmptySeries=false should skip stub series (not delete)
        head.closeHeadChunks(false);

        // Stub series should still exist (skipped, not deleted)
        MemSeries afterFlush = head.getSeriesMap().getByReference(ref);
        assertNotNull("Stub series should still exist when drop not allowed", afterFlush);
        assertTrue("Series should still be stub", afterFlush.isStub());
        assertEquals("Stub counter should still be 1", 1, head.getSeriesMap().getStubSeriesCount());

        head.close();
        cm.close();
    }

    public void testFlushDeletesOrphanedStubSeriesWhenDropAllowed() throws IOException, InterruptedException {
        ClosedChunkIndexManager cm = new ClosedChunkIndexManager(
            createTempDir("metrics"),
            new InMemoryMetadataStore(),
            new NOOPRetention(),
            new NoopCompaction(),
            threadPool,
            new ShardId("test", "uuid", 0),
            defaultSettings
        );
        Head head = new Head(createTempDir("head"), new ShardId("test", "uuid", 0), cm, defaultSettings);

        Labels labels = ByteLabels.fromStrings("k1", "v1");
        long ref = labels.stableHash();

        // Create a stub series with data during recovery
        Head.HeadAppender appender = head.newAppender();
        assertTrue(appender.preprocess(Engine.Operation.Origin.PEER_RECOVERY, 0, ref, null, 1000L, 100.0, () -> {}));
        appender.append(() -> {}, () -> {});

        // Verify series is a stub with data
        MemSeries series = head.getSeriesMap().getByReference(ref);
        assertTrue("Series should be stub", series.isStub());
        assertNull("Stub series should have null labels", series.getLabels());
        assertEquals("Stub counter should be 1", 1, head.getSeriesMap().getStubSeriesCount());

        // Flush with allowDropEmptySeries=true should delete orphaned stub series
        head.closeHeadChunks(true);

        // Stub series should be deleted (orphaned after recovery)
        MemSeries afterFlush = head.getSeriesMap().getByReference(ref);
        assertNull("Orphaned stub series should be deleted when drop allowed", afterFlush);
        assertEquals("Stub counter should be 0 after deletion", 0, head.getSeriesMap().getStubSeriesCount());

        head.close();
        cm.close();
    }

    /**
     * Helper method to query the LiveSeriesIndex and get the MIN_TIMESTAMP for a specific series reference.
     *
     * @param liveSeriesIndex the LiveSeriesIndex to query
     * @param ref the series reference to search for
     * @return the MIN_TIMESTAMP value for the series
     * @throws IOException if there's an error querying the index
     */
    private long getMinTimestampFromLiveSeriesIndex(LiveSeriesIndex liveSeriesIndex, long ref) throws IOException {
        ReaderManager readerManager = liveSeriesIndex.getDirectoryReaderManager();
        readerManager.maybeRefresh();
        DirectoryReader reader = null;
        try {
            reader = readerManager.acquire();
            IndexSearcher searcher = new IndexSearcher(reader);

            // Search for the specific reference
            org.apache.lucene.search.Query query = org.apache.lucene.document.LongPoint.newExactQuery(
                org.opensearch.tsdb.core.mapping.Constants.IndexSchema.REFERENCE,
                ref
            );
            TopDocs topDocs = searcher.search(query, 1);

            if (topDocs.scoreDocs.length == 0) {
                throw new IllegalStateException("No document found for reference: " + ref);
            }

            // Get MIN_TIMESTAMP for this document
            int docId = topDocs.scoreDocs[0].doc;
            LeafReaderContext leafContext = reader.leaves().get(org.apache.lucene.index.ReaderUtil.subIndex(docId, reader.leaves()));
            int localDocId = docId - leafContext.docBase;

            NumericDocValues minTimestampValues = leafContext.reader()
                .getNumericDocValues(org.opensearch.tsdb.core.mapping.Constants.IndexSchema.MIN_TIMESTAMP);
            if (minTimestampValues == null) {
                throw new IllegalStateException("MIN_TIMESTAMP field not found");
            }

            if (!minTimestampValues.advanceExact(localDocId)) {
                throw new IllegalStateException("MIN_TIMESTAMP value not found for document");
            }

            return minTimestampValues.longValue();
        } finally {
            if (reader != null) {
                readerManager.release(reader);
            }
        }
    }
}
